{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  },
  "lastEditStatus": {
   "notebookId": "l4sh3pspl652zlbqflq3",
   "authorId": "330324676895",
   "authorName": "ADMIN",
   "authorEmail": "aswinee.rath@snowflake.com",
   "sessionId": "f7742183-bb2f-47ec-add2-9f22c8dc5d01",
   "lastEditTime": 1760665526825
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "id": "3775908f-ca36-4846-8f38-5adca39217f2",
   "metadata": {
    "language": "python",
    "name": "import_packages"
   },
   "source": "# Import python packages\nimport streamlit as st\nimport pandas as pd\n\n# We can also use Snowpark for our analyses!\nfrom snowflake.snowpark.context import get_active_session\nsession = get_active_session()\n",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "8d50cbf4-0c8d-4950-86cb-114990437ac9",
   "metadata": {
    "language": "sql",
    "name": "set_db_context"
   },
   "source": "USE DATABASE DEMO_DB;\nUSE SCHEMA CRM;",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "c58b4bb6-3f12-4900-9012-c27d607d0afa",
   "metadata": {
    "language": "sql",
    "name": "trascript_data_preview"
   },
   "outputs": [],
   "source": "select * from CHAT_TRANSCRIPT limit 10;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "77bc0648-9779-487f-a674-d62bcfc68b75",
   "metadata": {
    "language": "sql",
    "name": "Knowledge_Base"
   },
   "outputs": [],
   "source": "select * from KNOWLEDGE_ARTICLES limit 5;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b0e98c7f-db7d-4e0e-9557-7bd4e15354a6",
   "metadata": {
    "language": "sql",
    "name": "Create_chunks_from_knowlege_table"
   },
   "outputs": [],
   "source": "-- Create chunks from knowledge article and give it index no \nCREATE OR REPLACE TABLE knowledge_chunks AS\nSELECT\n  q.articlenumber,                 -- add these in\n  q.title,\n  q.summary,\n  f.index + 1       AS chunk_id,      \n  f.value::string   AS chunk_text\nFROM KNOWLEDGE_ARTICLES AS q,\nLATERAL FLATTEN(\n  INPUT => SNOWFLAKE.CORTEX.SPLIT_TEXT_RECURSIVE_CHARACTER(\n             q.procedure_steps__c,           -- your text column\n             'none',             -- format (use 'CHARACTER' for char-based chunks)\n             1200,                    -- chunk size\n             100                     -- overlap (use 50 if you prefer)\n           )\n) AS f\nWHERE q.procedure_steps__c IS NOT NULL\nORDER BY chunk_id\n--limit 10\n;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "9c28deef-507f-467d-8f2e-1c454aa35d82",
   "metadata": {
    "language": "sql",
    "name": "Build_search_term"
   },
   "outputs": [],
   "source": "create or replace table chat_transcripts_kb as select *,\nAI_COMPLETE('claude-4-sonnet', 'You are a support search assistant. Given the following customer support transcript, output a contextual search query that will help locate the right knowledge article and action to take. Something like: What do I do when the customer had an order delivered to the wrong address? Do NOT include quotes or extra words. Transcript: ' || transcript) as search_term\nfrom CHAT_TRANSCRIPT;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "7d42ba83-f47b-4c17-a4a5-6f31808b4bcd",
   "metadata": {
    "language": "sql",
    "name": "Display_transcript_table"
   },
   "outputs": [],
   "source": "select * from CHAT_TRANSCRIPTS_KB limit 5;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8febf878-1549-49c6-83c0-776f665f4d75",
   "metadata": {
    "language": "sql",
    "name": "combine_transcript_knowlege_Article_chunk"
   },
   "outputs": [],
   "source": "CREATE OR REPLACE TABLE transcripts_with_chunks AS\nWITH base AS (\n  SELECT\n    t.chat_id,\n    k.articlenumber,\n    k.chunk_id,\n    k.chunk_text,\n    AI_SIMILARITY(\n      k.chunk_text,\n      t.search_term,\n      OBJECT_CONSTRUCT('model','voyage-multilingual-2')\n    ) AS sim\n  FROM chat_transcripts_kb t\n  JOIN knowledge_chunks k ON TRUE\n  WHERE t.search_term IS NOT NULL\n),\ntop5 AS (\n  SELECT *\n  FROM base\n  QUALIFY ROW_NUMBER() OVER (PARTITION BY chat_id ORDER BY sim DESC) <= 5\n),\nwindowed AS (\n  SELECT\n    t5.chat_id,\n    k2.articlenumber,\n    k2.chunk_id,\n    k2.chunk_text\n  FROM top5 t5\n  JOIN knowledge_chunks k2\n    ON k2.articlenumber = t5.articlenumber\n   AND k2.chunk_id BETWEEN t5.chunk_id - 1 AND t5.chunk_id + 1\n),\ndedup AS (\n  SELECT DISTINCT chat_id, articlenumber, chunk_id, chunk_text\n  FROM windowed\n)\nSELECT\n  chat_id,\n  ARRAY_AGG(\n    OBJECT_CONSTRUCT(\n      'article',    articlenumber,\n      'chunk_id',   chunk_id,\n      'Supporting_Knowledge_Details', chunk_text\n    )\n  ) WITHIN GROUP (ORDER BY articlenumber, chunk_id) AS kb_window_chunks\nFROM dedup\nGROUP BY chat_id;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b8c95056-17b9-4e64-be32-4d2534d067a3",
   "metadata": {
    "language": "sql",
    "name": "data_in_transcripts_with_chunks"
   },
   "outputs": [],
   "source": "select * from transcripts_with_chunks limit 5;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "5adc538b-6d54-429f-822c-afcf8fe03139",
   "metadata": {
    "language": "sql",
    "name": "Combine_chunk_table_Transcript"
   },
   "outputs": [],
   "source": "create or replace table retail_chat_transcripts_kb as \nselect RCT.*,TWC.KB_WINDOW_CHUNKS \nfrom TRANSCRIPTS_WITH_CHUNKS TWC , CHAT_TRANSCRIPTS_KB RCT\nwhere TWC.chat_id=RCT.chat_id;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d3293ca3-d7d3-431e-b30c-778d08503fcf",
   "metadata": {
    "language": "sql",
    "name": "all_combined_data"
   },
   "outputs": [],
   "source": "select * from retail_chat_transcripts_kb limit 5;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b3826e9d-6aed-47e1-a5c6-872a20153da1",
   "metadata": {
    "language": "sql",
    "name": "LLM_Scoring_rubrik_Prompt"
   },
   "outputs": [],
   "source": "CREATE OR REPLACE TABLE QA_SCORING_SUMMARY AS\nSELECT\n    T.AGENT,\n    T.CHAT_ID,\n    T.TRANSCRIPT,\n    T.KB_WINDOW_CHUNKS,      -- VARIANT or TEXT: concatenated or array of knowledge article texts/URLs\n    TO_VARIANT(CASE\n        WHEN T.TRANSCRIPT IS NULL OR TRIM(T.TRANSCRIPT) = ''\n        THEN NULL\n        ELSE AI_COMPLETE(\n        model => 'claude-4-sonnet',\n        prompt =>\n'## ROLE\nYou are a senior QA analyst evaluating call/chat interactions for Acme Corp. Score the interaction strictly against the rubric provided. Use ONLY the supplied sources (transcript, Salesforce case & item records, and knowledge articles). If a required detail is not present in the supplied sources, mark \"insufficient_evidence\": true for that element and score it 0 (do NOT guess).\n\n## CRITICAL JSON VALIDATION REQUIREMENTS\n- YOU MUST COPY THE EXACT JSON TEMPLATE BELOW AND ONLY CHANGE THE VALUES\n- NEVER omit any field from any element\n- EVERY element MUST have: \"name\", \"score\", \"max_score\", \"insufficient_evidence\", \"rationale\", \"evidence\"\n- \"insufficient_evidence\" is MANDATORY - set to true or false for EVERY element\n- If you cannot find evidence, set \"insufficient_evidence\": true and \"score\": 0\n\n## OUTPUT CONTRACT (CRITICAL)\nReturn ONE valid JSON object and nothing else. Follow the schema exactly. Every element MUST include a brief \"rationale\" and an \"evidence\" array of verbatim snippets (â‰¤25 words each) citing the exact source used: \"transcript\", \"sf_case\", \"sf_items\", \"kb\". If no evidence exists to support the scoring element, use an empty array and set \"insufficient_evidence\": true.\n\n\n## RUBRIC & ELEMENT DEFINITIONS\n\n### 3) Policy/Process Education\n> Sources: transcript + knowledge articles\n> Goal: Educator explained the resolution offered to the Guest and provided accurate education around Acme Corp policy for the resolution offered.\n> Scoring:\n- 2 pt: Educator explained the resolution offered accurately according to all available sources\n- 1 pt: Educator explained the resolution offered, BUT some of the information was either missing or inaccurate\n- 0 pt: Educator did not provide an explanation of the resolution\n** Always include a reference to the knowlege article in your rationale and evidence as support for providing the score and describe exactley what the educator should have done if not assigned 2 points, if there is ambiguity but the educator created a good guest experience, you can assign 2. \n***Be extra careful about the root cause. You may see that an order takes 1-2 days to process for shipping but then 2-6 days for actual shipping once processed. Do not get confused in these cases, be careful, identify the root cause, and then reference the appropriate operating guidelines when judging the agent.\n\n### 4) POWER UP: Education on Acme Corp offerings\n> Sources: transcript\n> Goal: The educator proactively provided the Guest education on Acme Corp offerings to promote Acme Corp community (i.e., beyond education on policy and timelines), such as: Describing membership programs, Highlighting community events, Highlighting available website / app self-service capabilities.\n> Scoring:\n1 pt - proactively promoted Acme Corp community offerings\n0 pt - just stayed on task, did not add any community commentary\n\n\n### 5) Timeline education \n> Sources: transcript + knowledge articles\n> Goal: Educator explained the timelines for the resolution offered to the Guest and provided accurate education around the timelines for the resolution offered\n> Scoring:\n- 2 pt: Educator provided accurate timelines for the resolution offered according to all available sources\n- 1 pt: Educator provided timelines for the resolution offered, but timelines were not accurate or correct\n- 0 pt: Educator did not provide education for timelines for the resolution\n** If assigning 0 or 1 point, state exactly what the agent should have done to achieve score of 2 in your rationale\n**Be extra careful about the root cause. You may see that an order takes 1-2 days to process for shipping but then 2-6 days for actual shipping once processed. Do not get confused in these cases, be careful, identify the root cause, and then reference the appropriate operating guidelines when judging the agent.\n\n###SOFT SKILLS###\n> Sources: transcript\n### 6) Introduction: Assign 1 pt if the educator introduces themselves by name, else 0.\n### 7) Connection: Assign 1 pt if the educator went above-and-beyond empathy/connection (uses guest name, references profile info, references prior info, or small-talk to connect)\n### 8) Spelling and grammar: Spelling/grammar/product-name accuracy; no slang/abbr.\n  - 2 pts: 0â€“3 errors\n  - 1 pts: 4â€“6 errors\n  - 0 pts: â‰¥7 errors\n### 9) Standard text/Quick replies: Assign 1pt if there are no obvious placeholders from templates left in communication, else 0.\n### 10) Tailored acknowledgement: Assign 1pt if educator tailors initial acknowledgement to reason of contact, else 0. Saying I can help is not a tailored acknowledgement. Tailored acknowledgement shows empathy and understanding. Example for a guest unable to cancel an order would be â€œIâ€™m sorry to hear you canâ€™t cancel your order, let me see how I can help\n\n\n### 11) Avoid repetition\n> Sources: transcript\n> Goal: Educator does not repeat previously asked for or discussed information\n> Scoring:\n- 1 pt: Avoids asking guest to repeat previously provided info; if repeated, acknowledges & thanks\n- 0 pt: Asks guests to repeat information and does not acknowledge the inconvenience\n\n### 12) Hold times\n> Sources: transcript\n> Goal: Enables good guest experience by providing a \"by-when\" before holds or research time\n> Scoring:\n- 1 pt: Always provides clear guidance if they need to take time or put the guest on hold and sets proper expectations that they adhere to.\n- 0 pt: If they do not provide notice of hold or long pauses or if they do not adhere to expectations. i.e. I will be back in a minute, and does not return for 3 minutes.\n\n### 13) Timely responses\n> Sources: transcript timestamps\n> Goal: Responds within channel expectations, Phone â‰¤ 3s, Live chat â‰¤ 10s, Apple for business â‰¤ 10s. (If timestamps unavailable, mark insufficient_evidence.)\n> Scoring\n- 1 pt: If ALWAYS responds according to SLAs, or is clear in justifying why when they are out of SLA\n- 0 pt: Misses SLAs and does not provide any justification to the guest\n\n### 14) PCI components validated\n> Sources: transcript only.\n> Goal: Educator validates â‰¥3 of 5 PCI components effectively (name, phone, order number, address, email). If there is prior chat history available where a previous agent already captured this information but the agent asked again, assign 0 points. \n> Scoring: \n1 pt - if 3 or more of the 5 components are captured\n0 pt - if less than 3 of 5 are captured or the guest was asked repeated information.\n\n### 15) Product-related offering\n> Sources: transcript\n> Goal: educator proactively drives revenue by making an effort to place another order, recommending products, suggesting DE appointment, explaining Science of Feel etc.\n> Scoring:\n1 pt - proactively drove incremental revenue in anyway. Offering to re place an order counts.\n0 pt - just stayed on task, did not make any new offers\n\n### 16) Resolution Offered\n> Sources: transcript + knowledge articles\n> Goal: Educator supported the Guests needs by offering a resolution that aligns to Acme Corp policy, training, and/or knowledge base, or providing a resolution that supports Guest experience when necessary for an identified exception in SOP \n> Scoring: \n2 pt - if the resolution aligns to policy/training/KB OR if its justified by exception because the agent pursued the best possible Guest experience. \n1 pt - as long as an appropriate resolution was offered\n0 pt - if no resolution or a completely incorrect resolution was provided\n\n### 17) Resolution set-up\n> Sources: transcript + knowledge articles + item records\n> Goal: Educator accurately created the Reason of Contact in Salesforce\n> Scoring:\n2 pt - Correct Reason of Contact is listed in Salesforce case\n1 pt - If reason of contact is relatively correct\n0 pt - If reason of contact is wrong\n\n###System Use Scores###\n> Sources: salesforce case record\n### 19) Reason of Contact: Assign 1 pt if the reason of contact field correctly reflects the conversation, else 0.\n### 20) Case Status: Assign 1pt if case status correct (status is \"closed\" if no follow-up; \"waiting on guest\" if guest follow-up needed), else 0.\n### 21) Case Subject Line: Assign 1pt if subject field accurately reflects purpose, else 0.\n### 22) Guest Profile: Assign 1pt if phone in case matches transcript mention (if phone was mentioned), else 0.\n### 23) Guest shipping address: Assign 1pt if address captured correctly IF the guest shared it in the interaction, else 0.\n### 24) Supplementary Items: Assign 1pt if item records reflect all actions, questions, and suggestions, else 0.\n### 25) Timeline notes: Assign 1pt if case notes accurately capture the call and resolution, else 0.\n\n## INPUTS\nYou will receive:\n- transcript: full text of the interaction\n- sf_case: JSON of the Salesforce case record\n- kb_articles: concatenated text or array of relevant knowledge articles (policy/SOP)\n\n## DECISION RULES TO REDUCE HALLUCINATIONS\n- Cite exact snippets for every non-binary claim.\n- If a required datum is not explicitly present in the provided sources, set \"insufficient_evidence\": true and score 0 for that element; do NOT infer from brand common sense.\n- For timestamps/SLAs, only use explicit timing from transcript metadata; if none, mark insufficient_evidence.\n- For policy alignment, quote the specific KB passage used.\n\n## REQUIRED JSON TEMPLATE - COPY EXACTLY AND FILL VALUES\nIMPORTANT: Copy this exact structure. Do not omit any field. Every element must have \"insufficient_evidence\": true or false.\n{\n  \"agent_persona_summary\": \"1â€“3 sentences on strengths and weaknesses of this interaction backing up the scores provided.\",\n  \"elements\": [\n        {\n          \"name\": \"<insert the name of the scoring element>\",\n          \"element_id\": \"<insert the ID/number of the corresponding scoring element>\",\n          \"rationale\": \"<REQUIRED: Brief explanation of scoring decision based on the rubric definition>\",\n          \"score\": 0,\n          \"insufficient_evidence\": <boolean true/false value if evidence was present to support the score>,\n          \"evidence\": [<provide the supporting snippet from the transcript, knowledge article, or case value that backs up the scoring rationale>]\n        },\n        ...\n      ]\n}\n  \n## FINAL VALIDATION CHECKLIST - CRITICAL\nBefore returning your JSON, verify EVERY SINGLE ELEMENT has these 6 fields:\n1. \"name\" (string)\n2. \"element_id\" (string)\n3. \"score\" (integer) \n4. \"insufficient_evidence\" (boolean - true or false)\n5. \"rationale\" (string)\n6. \"evidence\" (array of strings)\n\nMISSING ANY OF THESE FIELDS WILL CAUSE AN ERROR. Check every element in every category.\n\n## SOURCE DATA TO USE FOR THE ASSESSMENT\n# TRANSCRIPT\n<<TRANSCRIPT_START>>\n' || COALESCE(T.transcript, 'No transcript available') || '\n<<TRANSCRIPT_END>>\n\n\n\n# KNOWLEDGE ARTICLES (TEXT OR JSON)\n<<KB_ARTICLES_START>>\n' || to_json(T.KB_WINDOW_CHUNKS)::STRING || '\n<<KB_ARTICLES_END>>',\n  response_format => {\n    'type': 'json',\n    'schema': {\n      'type': 'object',\n      'properties': {\n        'agent_persona_summary': { 'type': 'string' },\n        'elements': {\n          'type': 'array',\n          'items': {\n            'type': 'object',\n            'properties': {\n              'name': { 'type': 'string' },\n              'element_id': { 'type': 'string' },\n              'rationale': { 'type': 'string' },\n              'score': { 'type': 'integer' },\n              'insufficient_evidence': { 'type': 'boolean' },\n              'evidence': {\n                'type': 'array',\n                'items': { 'type': 'string' }\n              }\n            },\n            'required': [\n              'name',\n              'element_id',\n              'rationale',\n              'score',\n              'insufficient_evidence',\n              'evidence'\n            ],\n            'additionalProperties': false\n          }\n        }\n      },\n      'required': ['agent_persona_summary', 'elements'],\n      'additionalProperties': false\n    }\n  },\n        model_parameters => {\n            'temperature': 0\n        }\n        )\n    END ) AS qa_score\nFROM\n    retail_chat_transcripts_kb AS T;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "5634fd95-080b-414f-a8d1-fb1395b40f37",
   "metadata": {
    "language": "sql",
    "name": "Review_Rubric_llm_scoring"
   },
   "outputs": [],
   "source": "select * from QA_SCORING_SUMMARY limit 5;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "355e8e0c-a168-48ed-b011-0ac53bc4f2f5",
   "metadata": {
    "language": "sql",
    "name": "Manual_Score_Provided_GEC_Team"
   },
   "outputs": [],
   "source": "select * from HUMAN_QM_SCORES limit 5;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "db4f2aaf-e596-4ac9-b75c-0746ff1ee92a",
   "metadata": {
    "language": "sql",
    "name": "LLM_Human_Score_Alignment_View"
   },
   "outputs": [],
   "source": "create or replace view SCORING_VIEW\nas\nWITH llm AS (\n  SELECT\n      q.chat_id,\n      q.agent,\n      q.transcript,\n      q.KB_WINDOW_CHUNKS,\n      q.qa_score:\"agent_persona_summary\"::string AS agent_persona_summary,\n      e.value:\"element_id\"::string               AS element_id,\n      e.value:\"name\"::string                     AS element_name,        -- col1\n      e.value:\"score\"::number                    AS llm_score,           -- col2\n      e.value:\"rationale\"::string                AS element_rationale,\n      e.value:\"insufficient_evidence\"::boolean   AS insufficient_evidence,\n      e.value:\"evidence\"                         AS evidence             -- array/VARIANT\n  FROM qa_scoring_summary q,\n       LATERAL FLATTEN(input => q.qa_score:\"elements\") e\n)\nSELECT distinct\n    llm.element_name                             AS element_name,\n    llm.element_id                               AS element_id,    \n    llm.llm_score                                AS llm_score,\n    TRY_TO_NUMBER(h.score)                       AS human_score,\n    (llm.llm_score = TRY_TO_NUMBER(h.score))     AS scores_match,\n     TRY_TO_NUMBER(h.possible_score)             AS possible_max_score,\n    llm.insufficient_evidence                    AS insufficient_evidence,\n    llm.element_rationale                        AS rationale,\n    llm.evidence                                 AS evidence,\n    llm.agent                                    AS agent,\n    llm.chat_id                                  AS chat_id,\n    llm.agent_persona_summary                    AS agent_persona_summary,\n    llm.transcript                               AS transcript,\n    llm.KB_WINDOW_CHUNKS\nFROM llm\nLEFT JOIN HUMAN_QM_SCORES h\n  ON h.chat_id = llm.chat_id\n  and h.element_id = llm.element_id\nORDER BY llm.chat_id, llm.element_id ASC;\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "685d9f39-2048-41b5-a30f-4c38a0b7580f",
   "metadata": {
    "language": "sql",
    "name": "data_used_in_our_dashboards"
   },
   "outputs": [],
   "source": "select * from SCORING_VIEW limit 10;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "97c6c7c8-1b6d-45c7-9f17-d385eea4b278",
   "metadata": {
    "name": "addl_analytics",
    "collapsed": false
   },
   "source": "## Additional analyis"
  },
  {
   "cell_type": "code",
   "id": "8211ed8f-7c8f-44e4-951d-7f20b23f6b54",
   "metadata": {
    "language": "sql",
    "name": "Additional_Insights_about_bots"
   },
   "outputs": [],
   "source": "--CREATE OR REPLACE TABLE insights AS\nSELECT\n    T.AGENT,\n    T.CHAT_ID,\n    T.TRANSCRIPT,\n    TO_VARIANT(CASE\n        WHEN T.TRANSCRIPT IS NULL OR TRIM(T.TRANSCRIPT) = ''\n        THEN NULL\n        ELSE AI_COMPLETE(\n        model => 'claude-4-sonnet',\n        prompt =>\n'## ROLE\nYou are a senior QA analyst evaluating call/chat interactions for Acme Corp. Given the call transcript please output the following details in JSON format, no preceeding or trailing characters in the {} json response.\n\n## REQUIRED JSON TEMPLATE - COPY EXACTLY AND FILL VALUES\nIMPORTANT: Copy this exact structure. Do not omit any field.\n{\n  \"llm_reason_of_contact\": <2-3 word reason for call>,\n  \"guest_dropped\": <boolean value representing if the guest dropped from the conversation. You will see evidence of the educator asking if guest is still there>,\n  \"guest_dropped_detail\": <1 sentence explanation of how and ideally why the guest dropped the conversation. If guest did not drop, leave blank>,\n  \"guest_frustrated_w_bot\": <boolean value representing whether the guest was frustrated with the bot before being transferred to a live educator>,\n  \"guest_frustrated_w_bot_detail\": <1-2 sentences explaining why the bot was furstrating the customer. If guest was not frustrated, leave blank.>\n}\n\n# SOURCE DATA TO USE FOR THE ASSESSMENT\n## TRANSCRIPT\n<<TRANSCRIPT_START>>\n' || COALESCE(T.transcript, 'No transcript available') || '\n<<TRANSCRIPT_END>>\n',\n  response_format => {\n    'type': 'json',\n    'schema': {\n      'type': 'object',\n      'properties': {\n        'llm_reason_of_contact': {'type': 'string'},\n        'guest_dropped' : {'type': 'boolean'},\n        'guest_dropped_detail': {'type': 'string'},\n        'guest_frustrated_w_bot': {'type': 'boolean'},\n        'guest_frustrated_w_bot_detail': {'type': 'string'}\n      },\n      'required': ['llm_reason_of_contact', 'guest_dropped', 'guest_frustrated_w_bot', 'guest_frustrated_w_bot_detail','guest_dropped_detail' ],\n      'additionalProperties': false\n    }\n  },\n        model_parameters => {\n            'temperature': 0\n        }\n        )\n    END ) AS insights_output\nFROM\n    retail_chat_transcripts_kb AS T\nLIMIT 1;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3ca1f570-993b-4326-8dd2-25ee38f78d25",
   "metadata": {
    "language": "sql",
    "name": "Additional_Insights_flattened"
   },
   "outputs": [],
   "source": "CREATE OR REPLACE VIEW insights_flattened AS\nSELECT\n    AGENT,\n    CHAT_ID,\n    TRANSCRIPT,\n    SF_CASE,\n    REASON_OF_CONTACT__C,\n    STATUS,\n    SUBJECT,\n    insights_output:\"llm_reason_of_contact\"::STRING AS llm_reason_of_contact,\n    insights_output:\"guest_dropped\"::BOOLEAN AS guest_dropped,\n    insights_output:\"guest_dropped_detail\"::STRING AS guest_dropped_detail,\n    insights_output:\"guest_frustrated_w_bot\"::BOOLEAN AS guest_frustrated_w_bot,\n    insights_output:\"guest_frustrated_w_bot_detail\"::STRING AS guest_frustrated_w_bot_detail\nFROM insights;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "840116a3-6770-49e0-a7f0-5a2af23106a6",
   "metadata": {
    "language": "sql",
    "name": "cell18"
   },
   "outputs": [],
   "source": "select * from InSIGHTS_FLATTENED ;",
   "execution_count": null
  }
 ]
}